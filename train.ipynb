{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "new_train.ipynb",
      "provenance": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UAK9g6arilE9",
        "outputId": "013aec28-98ce-4d87-9885-656a8a0ba47a"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/gdrive')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Rfhr1MMthLQm"
      },
      "source": [
        "EPOCHS = 100\n",
        "WEIGHT_DECAY = 5e-4\n",
        "FAMILY_WEIGHT = 0.05\n",
        "IDENTITY_WEIGHT = 0.05\n",
        "LEARNING_RATE = 0.01\n",
        "MULTI_STEP_LR_DECAY = [30, 60, 90]\n",
        "DATASET = 'original' # choice = ['original', 'SR9', 'SR20']\n"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uJDUaYmUhtcF"
      },
      "source": [
        "import os\n",
        "import math\n",
        "import pickle\n",
        "import cv2\n",
        "from glob import glob\n",
        "\n",
        "import torch\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "\n",
        "from PIL import Image\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torchvision import transforms, models\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.image as img\n",
        "\n",
        "class TrainDataset(Dataset):\n",
        "    def __init__(self, transform = None):\n",
        "      # Save the entire path for data (list and label as well)\n",
        "        super(TrainDataset, self).__init__()\n",
        "        self.transform = transform\n",
        "\n",
        "        # Instead of local drive, I am using files in gdrive mounted\n",
        "        #self.all_images = glob(DATASET + '/train/train-faces/' + \"*/*/*.jpg\") # original\n",
        "        self.all_images = glob('/gdrive/MyDrive/Kinship Recognition Starter/train/train-faces/' + \"*/*/*.jpg\") # original\n",
        "\n",
        "        self.img_pathes = []\n",
        "        \n",
        "        all_classes = [s.split('/')[-3] for s in self.all_images] # family name list\n",
        "        self.class_index = list(set(all_classes)) # family index\n",
        "        \n",
        "        all_classes2 = [s.split('/')[-3] + '_' + s.split('/')[-2] for s in self.all_images] # identity name list\n",
        "        self.class_index2 = list(set(all_classes2)) # identity index\n",
        "        \n",
        "        self.classes = [self.class_index.index(name) for name in all_classes] # family label for each sample\n",
        "        self.classes2 = [self.class_index2.index(name) for name in all_classes2] # identity label for each sample\n",
        "        \n",
        "    def __len__(self):\n",
        "      # the size of the entire dataset\n",
        "        return len(self.all_images)\n",
        "        \n",
        "    def __getitem__(self, idx):\n",
        "      # function that returns data x, label one by one when there is an input idx\n",
        "        img_path = self.all_images[idx]\n",
        "        img = Image.open(img_path).convert('RGB')\n",
        "\n",
        "        if self.transform is not None: # augmentations\n",
        "            img = self.transform(img)\n",
        "                    \n",
        "        if img.size(0) == 1: # if greyscale, repeat\n",
        "            img = img.repeat(3, 1, 1)\n",
        "            \n",
        "        # 3 C(RGB) // 1 Gray(black) x Height x Weight\n",
        "        family_label = torch.LongTensor([self.classes[idx]]) # family label\n",
        "        identity_label = torch.LongTensor([self.classes2[idx]]) #\n",
        "\n",
        "        return img, family_label, identity_label"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f-41mij7ifUY"
      },
      "source": [
        "# Reference: https://github.com/cydonia999/VGGFace2-pytorch\n",
        "# ZQ. Cao, L. Shen, W. Xie, O. M. Parkhi, A. Zisserman, VGGFace2: A dataset for recognising faces across pose and age, 2018.\n",
        "# https://arxiv.org/pdf/1710.08092.pdf\n",
        "\n",
        "def conv3x3(in_planes, out_planes, stride=1):\n",
        "    \"\"\"3x3 convolution with padding\"\"\"\n",
        "    return nn.Conv2d(in_planes, out_planes, kernel_size=3, stride=stride,\n",
        "                     padding=1, bias=False)\n",
        "\n",
        "class SEModule(nn.Module):\n",
        "\n",
        "    def __init__(self, planes, compress_rate):\n",
        "        super(SEModule, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(planes, planes // compress_rate, kernel_size=1, stride=1, bias=True)\n",
        "        self.conv2 = nn.Conv2d(planes // compress_rate, planes, kernel_size=1, stride=1, bias=True)\n",
        "        self.relu = nn.ReLU(inplace=True)\n",
        "        self.sigmoid = nn.Sigmoid()\n",
        "\n",
        "    def forward(self, x):\n",
        "        module_input = x\n",
        "        x = F.avg_pool2d(module_input, kernel_size=module_input.size(2))\n",
        "        x = self.conv1(x)\n",
        "        x = self.relu(x)\n",
        "        x = self.conv2(x)\n",
        "        x = self.sigmoid(x)\n",
        "        return module_input * x\n",
        "\n",
        "\n",
        "class BasicBlock(nn.Module):\n",
        "    expansion = 1\n",
        "\n",
        "    def __init__(self, inplanes, planes, stride=1, downsample=None):\n",
        "        super(BasicBlock, self).__init__()\n",
        "        self.conv1 = conv3x3(inplanes, planes, stride)\n",
        "        self.bn1 = nn.BatchNorm2d(planes)\n",
        "        self.relu = nn.ReLU(inplace=True)\n",
        "        self.conv2 = conv3x3(planes, planes)\n",
        "        self.bn2 = nn.BatchNorm2d(planes)\n",
        "        self.downsample = downsample\n",
        "        self.stride = stride\n",
        "\n",
        "    def forward(self, x):\n",
        "        residual = x\n",
        "\n",
        "        out = self.conv1(x)\n",
        "        out = self.bn1(out)\n",
        "        out = self.relu(out)\n",
        "\n",
        "        out = self.conv2(out)\n",
        "        out = self.bn2(out)\n",
        "\n",
        "        if self.downsample is not None:\n",
        "            residual = self.downsample(x)\n",
        "\n",
        "        out += residual\n",
        "        out = self.relu(out)\n",
        "\n",
        "        return out\n",
        "\n",
        "\n",
        "class Bottleneck(nn.Module):\n",
        "    expansion = 4\n",
        "\n",
        "    def __init__(self, inplanes, planes, stride=1, downsample=None):\n",
        "        super(Bottleneck, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(inplanes, planes, kernel_size=1, stride=stride, bias=False)\n",
        "        self.bn1 = nn.BatchNorm2d(planes)\n",
        "        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3, stride=1, padding=1, bias=False)\n",
        "        self.bn2 = nn.BatchNorm2d(planes)\n",
        "        self.conv3 = nn.Conv2d(planes, planes * 4, kernel_size=1, bias=False)\n",
        "        self.bn3 = nn.BatchNorm2d(planes * 4)\n",
        "        self.relu = nn.ReLU(inplace=True)\n",
        "        self.downsample = downsample\n",
        "        self.stride = stride\n",
        "\n",
        "        # SENet\n",
        "        compress_rate = 16\n",
        "        # self.se_block = SEModule(planes * 4, compress_rate)  # this is not used.\n",
        "        self.conv4 = nn.Conv2d(planes * 4, planes * 4 // compress_rate, kernel_size=1, stride=1, bias=True)\n",
        "        self.conv5 = nn.Conv2d(planes * 4 // compress_rate, planes * 4, kernel_size=1, stride=1, bias=True)\n",
        "        self.sigmoid = nn.Sigmoid()\n",
        "\n",
        "    def forward(self, x):\n",
        "        residual = x\n",
        "\n",
        "        out = self.conv1(x)\n",
        "        out = self.bn1(out)\n",
        "        out = self.relu(out)\n",
        "\n",
        "        out = self.conv2(out)\n",
        "        out = self.bn2(out)\n",
        "        out = self.relu(out)\n",
        "\n",
        "        out = self.conv3(out)\n",
        "        out = self.bn3(out)\n",
        "\n",
        "\n",
        "        ## senet\n",
        "        out2 = F.avg_pool2d(out, kernel_size=out.size(2))\n",
        "        out2 = self.conv4(out2)\n",
        "        out2 = self.relu(out2)\n",
        "        out2 = self.conv5(out2)\n",
        "        out2 = self.sigmoid(out2)\n",
        "        # out2 = self.se_block.forward(out)  # not used\n",
        "\n",
        "        if self.downsample is not None:\n",
        "            residual = self.downsample(x)\n",
        "\n",
        "        out = out2 * out + residual\n",
        "        # out = out2 + residual  # not used\n",
        "        out = self.relu(out)\n",
        "        return out\n",
        "\n",
        "\n",
        "class SENet(nn.Module):\n",
        "\n",
        "    def __init__(self, block, layers, num_classes=1000, include_top=True):\n",
        "        self.inplanes = 64\n",
        "        super(SENet, self).__init__()\n",
        "        self.include_top = include_top\n",
        "        \n",
        "        self.conv1 = nn.Conv2d(3, 64, kernel_size=7, stride=2, padding=3, bias=False)\n",
        "        self.bn1 = nn.BatchNorm2d(64)\n",
        "        self.relu = nn.ReLU(inplace=True)\n",
        "        self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=0, ceil_mode=True)\n",
        "\n",
        "        self.layer1 = self._make_layer(block, 64, layers[0])\n",
        "        self.layer2 = self._make_layer(block, 128, layers[1], stride=2)\n",
        "        self.layer3 = self._make_layer(block, 256, layers[2], stride=2)\n",
        "        self.layer4 = self._make_layer(block, 512, layers[3], stride=2)\n",
        "        self.avgpool = nn.AvgPool2d(7, stride=1)\n",
        "        self.fc = nn.Linear(512 * block.expansion, num_classes)\n",
        "\n",
        "        for m in self.modules():\n",
        "            if isinstance(m, nn.Conv2d):\n",
        "                n = m.kernel_size[0] * m.kernel_size[1] * m.out_channels\n",
        "                m.weight.data.normal_(0, math.sqrt(2. / n))\n",
        "            elif isinstance(m, nn.BatchNorm2d):\n",
        "                m.weight.data.fill_(1)\n",
        "                m.bias.data.zero_()\n",
        "\n",
        "    def _make_layer(self, block, planes, blocks, stride=1):\n",
        "        downsample = None\n",
        "        if stride != 1 or self.inplanes != planes * block.expansion:\n",
        "            downsample = nn.Sequential(\n",
        "                nn.Conv2d(self.inplanes, planes * block.expansion,\n",
        "                          kernel_size=1, stride=stride, bias=False),\n",
        "                nn.BatchNorm2d(planes * block.expansion),\n",
        "            )\n",
        "\n",
        "        layers = []\n",
        "        layers.append(block(self.inplanes, planes, stride, downsample))\n",
        "        self.inplanes = planes * block.expansion\n",
        "        for i in range(1, blocks):\n",
        "            layers.append(block(self.inplanes, planes))\n",
        "\n",
        "        return nn.Sequential(*layers)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.conv1(x)\n",
        "        x = self.bn1(x)\n",
        "        x = self.relu(x)\n",
        "        x = self.maxpool(x)\n",
        "        x = self.layer1(x)\n",
        "        x = self.layer2(x)\n",
        "        x = self.layer3(x)\n",
        "        x = self.layer4(x)\n",
        "\n",
        "        x = self.avgpool(x)\n",
        "        \n",
        "        if not self.include_top:\n",
        "            return x\n",
        "        \n",
        "        x = x.view(x.size(0), -1)\n",
        "        x = self.fc(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "def senet50(**kwargs):\n",
        "    \"\"\"Constructs a SENet-50 model.\n",
        "    \"\"\"\n",
        "    model = SENet(Bottleneck, [3, 4, 6, 3], **kwargs)\n",
        "    return model\n",
        "\n",
        "\n",
        "def load_state_dict(model, fname):\n",
        "    \"\"\"\n",
        "    Set parameters converted from Caffe models authors of VGGFace2 provide.\n",
        "    See https://www.robots.ox.ac.uk/~vgg/data/vgg_face2/.\n",
        "    Arguments:\n",
        "        model: model\n",
        "        fname: file name of parameters converted from a Caffe model, assuming the file format is Pickle.\n",
        "    \"\"\"\n",
        "    with open(fname, 'rb') as f:\n",
        "        weights = pickle.load(f, encoding='latin1')\n",
        "\n",
        "    own_state = model.state_dict()\n",
        "    for name, param in weights.items():\n",
        "        if name in own_state:\n",
        "            try:\n",
        "                own_state[name].copy_(torch.from_numpy(param))\n",
        "            except Exception:\n",
        "                raise RuntimeError('While copying the parameter named {}, whose dimensions in the model are {} and whose '\\\n",
        "                                   'dimensions in the checkpoint are {}.'.format(name, own_state[name].size(), param.size()))\n",
        "        else:\n",
        "            raise KeyError('unexpected key \"{}\" in state_dict'.format(name))"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qRFORR_HkG6r"
      },
      "source": [
        "class BaseModel(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(BaseModel, self).__init__()\n",
        "        N_IDENTITY = 8631\n",
        "        include_top = True\n",
        "        self.backbone = senet50(num_classes=N_IDENTITY, include_top=include_top)\n",
        "        load_state_dict(self.backbone, '/gdrive/MyDrive/Kinship Recognition Starter/senet50_ft_weight.pkl')\n",
        "        self.backbone.fc = nn.Linear(2048, 1024) # reset top layer\n",
        "        self.embedding = nn.Linear(1024, 512)\n",
        "        self.family_classifier = nn.Linear(1024, 192) \n",
        "        self.identity_classifier = nn.Linear(1024, 966)\n",
        "        \n",
        "    def forward(self, x):\n",
        "        feature = self.backbone(x)\n",
        "        feature = F.relu(feature)\n",
        "        \n",
        "        embedding = self.embedding(feature)\n",
        "        embedding = F.normalize(embedding, p=2)\n",
        "        \n",
        "        family_pred = self.family_classifier(feature)\n",
        "        identity_pred = self.identity_classifier(feature)\n",
        "        \n",
        "        return embedding, family_pred, identity_pred"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d1cFA3GblJvs"
      },
      "source": [
        "def pos_neg_mask(family_label):\n",
        "    family_label = family_label.squeeze()\n",
        "    pos_mask = family_label.unsqueeze(0) == family_label.unsqueeze(1) # size: Batch x Batch\n",
        "    pos_mask = pos_mask.float() # each index of row is anchor, and the other sample has the same family label\n",
        "    # positive element value is 1\n",
        "\n",
        "    neg_mask = 1 - pos_mask # \n",
        "    pos_mask -= torch.eye(family_label.size(0), device=family_label.device).float() # I matrix\n",
        "    return pos_mask, neg_mask\n",
        "\n",
        "# using anchor positive, negative indexes\n",
        "def triplet_sampling(pos_mask, neg_mask):\n",
        "    pos_pair_idx = pos_mask.nonzero()\n",
        "    apns = []\n",
        "    for pair_idx in pos_pair_idx:\n",
        "        anchor_idx = pair_idx[0]\n",
        "        neg_indices = neg_mask[anchor_idx].nonzero()\n",
        "\n",
        "        apn = torch.cat(\n",
        "            (pair_idx.unsqueeze(0).repeat(len(neg_indices), 1), neg_indices),\n",
        "            dim=1,\n",
        "        )\n",
        "        apns.append(apn)\n",
        "    apns = torch.cat(apns, dim=0)\n",
        "    anchor_idx = apns[:, 0]\n",
        "    pos_idx = apns[:, 1]\n",
        "    neg_idx = apns[:, 2]\n",
        "\n",
        "    return anchor_idx, pos_idx, neg_idx\n",
        "\n",
        "# define triplet loss\n",
        "class Triplet(torch.nn.Module):\n",
        "    def __init__(self, p=2, margin=0.2):\n",
        "        super().__init__()\n",
        "        self.p = p\n",
        "        self.margin = margin\n",
        "\n",
        "    # with mask, find triplet loss\n",
        "    def forward(self, embeddings, targets):\n",
        "        pos_mask, neg_mask = pos_neg_mask(targets)\n",
        "        anchor_idx, pos_idx, neg_idx = triplet_sampling(pos_mask, neg_mask)\n",
        "        if pos_idx is None:\n",
        "            return 0\n",
        "\n",
        "        anchor_embed = embeddings[anchor_idx]\n",
        "        positive_embed = embeddings[pos_idx]\n",
        "        negative_embed = embeddings[neg_idx]\n",
        "\n",
        "        loss = torch.nn.functional.triplet_margin_loss(\n",
        "            anchor_embed,\n",
        "            positive_embed,\n",
        "            negative_embed,\n",
        "            margin=self.margin, # margin = triplet margin\n",
        "            p=self.p, # p = p-norm\n",
        "            reduction=\"mean\",\n",
        "        )                \n",
        "        return loss"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-T_oKLmJmqgh",
        "outputId": "64c9aedf-2edd-4a6b-86bd-067dd66a7ede"
      },
      "source": [
        "model = BaseModel()\n",
        "model.cuda()\n",
        "#model.load_state_dict(torch.load(model_path)) # for fine-tune\n",
        "\n",
        "train_dataset = TrainDataset(transform=transforms.Compose([\n",
        "        transforms.Resize(256),\n",
        "        transforms.RandomCrop(224),\n",
        "        transforms.RandomGrayscale(p=0.2),\n",
        "        transforms.RandomHorizontalFlip(p=0.5),\n",
        "        transforms.ColorJitter(brightness=(0.8, 1.2), contrast=(0.8, 1.2), saturation=(0.8, 1.2)),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize((131.0912/255., 103.8827/255., 91.4953/255.), (1, 1, 1))\n",
        "    ]))\n",
        "\n",
        "train_loader = DataLoader(train_dataset, num_workers=4, batch_size=64, shuffle=True)\n",
        "\n",
        "criterion = Triplet()\n",
        "classifier_criterion = torch.nn.CrossEntropyLoss()\n",
        "\n",
        "model.train()\n",
        "\n",
        "num_epoch = EPOCHS\n",
        "family_weight = FAMILY_WEIGHT\n",
        "identity_weight = IDENTITY_WEIGHT\n",
        "weight_decay = WEIGHT_DECAY\n",
        "\n",
        "params = list(model.backbone.fc.parameters()) + list(model.embedding.parameters()) + list(model.family_classifier.parameters()) + list(model.identity_classifier.parameters()) # for training only new layers\n",
        "#params = list(model.parameters()) # for fine tuning\n",
        "optimizer = optim.SGD(params, lr=LEARNING_RATE, momentum=0.9, weight_decay=weight_decay)\n",
        "scheduler = optim.lr_scheduler.MultiStepLR(optimizer, MULTI_STEP_LR_DECAY, gamma=0.1)\n",
        "\n",
        "train_losses = []\n",
        "\n",
        "for epoch in range(num_epoch):\n",
        "    total_loss = 0.\n",
        "    total_tri_loss = 0.\n",
        "    total_family_loss = 0.\n",
        "    total_identity_loss = 0.\n",
        "    for batch_idx, (data, family_label, identity_label) in enumerate(train_loader):\n",
        "        data, family_label, identity_label = data.cuda(), family_label.cuda(), identity_label.cuda()\n",
        "        family_label, identity_label = family_label.squeeze(), identity_label.squeeze()\n",
        "\n",
        "        optimizer.zero_grad() \n",
        "        embedding, family_pred, identity_pred = model(data) # size : Batch x 512\n",
        "        tri_loss = criterion(embedding, family_label)\n",
        "        family_loss = classifier_criterion(family_pred, family_label) \n",
        "        identity_loss = classifier_criterion(identity_pred, identity_label)\n",
        "\n",
        "        loss = tri_loss + family_loss * family_weight + identity_loss * identity_weight\n",
        "        loss.backward()\n",
        "\n",
        "        total_tri_loss += tri_loss.item() * data.size(0)\n",
        "        total_family_loss += family_loss.item() * data.size(0)\n",
        "        total_identity_loss += identity_loss.item() * data.size(0)\n",
        "\n",
        "        total_loss += loss.item() * data.size(0)\n",
        "\n",
        "        optimizer.step()\n",
        "\n",
        "    scheduler.step()\n",
        "\n",
        "    total_loss /= 5045 # 5045 -> the # of data points\n",
        "    total_tri_loss /= 5045\n",
        "    total_family_loss /= 5045\n",
        "    total_identity_loss /= 5045\n",
        "    train_losses.append(total_loss)\n",
        "    print (\"Epoch {}, total_loss: {}, total_tri_loss: {}, total_family_loss: {}, total_identity_loss: {}\".format(epoch, total_loss, total_tri_loss, total_family_loss, total_identity_loss))"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:718: UserWarning: Named tensors and all their associated APIs are an experimental feature and subject to change. Please do not use them for anything important until they are released as stable. (Triggered internally at  /pytorch/c10/core/TensorImpl.h:1156.)\n",
            "  return torch.max_pool2d(input, kernel_size, stride, padding, dilation, ceil_mode)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 0, total_loss: 0.6925695864131122, total_tri_loss: 0.10306789269237027, total_family_loss: 4.983549163126496, total_identity_loss: 6.806484489894599\n",
            "Epoch 1, total_loss: 0.5629363658052488, total_tri_loss: 0.07210656616567265, total_family_loss: 3.891225303341778, total_identity_loss: 5.925370483379534\n",
            "Epoch 2, total_loss: 0.4503195798905092, total_tri_loss: 0.0595680910416457, total_family_loss: 2.8929821432168468, total_identity_loss: 4.922047475355468\n",
            "Epoch 3, total_loss: 0.3604497375407942, total_tri_loss: 0.05824776625978817, total_family_loss: 2.126397561979719, total_identity_loss: 3.917641776475254\n",
            "Epoch 4, total_loss: 0.2911179948432004, total_tri_loss: 0.055766846258765525, total_family_loss: 1.6234388290713397, total_identity_loss: 3.083584017843392\n",
            "Epoch 5, total_loss: 0.2333377222298867, total_tri_loss: 0.04839645231768445, total_family_loss: 1.2771272685293873, total_identity_loss: 2.4216981072137567\n",
            "Epoch 6, total_loss: 0.1999078539719785, total_tri_loss: 0.0513240846839426, total_family_loss: 1.0458389672580866, total_identity_loss: 1.9258363663500435\n",
            "Epoch 7, total_loss: 0.16806646505715944, total_tri_loss: 0.045459125962614186, total_family_loss: 0.8928842423574185, total_identity_loss: 1.559262479143171\n",
            "Epoch 8, total_loss: 0.14452840978255713, total_tri_loss: 0.04301448155294207, total_family_loss: 0.7517060091757798, total_identity_loss: 1.278572514981061\n",
            "Epoch 9, total_loss: 0.12951154697343073, total_tri_loss: 0.04371216248119078, total_family_loss: 0.6504357196530928, total_identity_loss: 1.065551941742155\n",
            "Epoch 10, total_loss: 0.11386532566858118, total_tri_loss: 0.03976825112430794, total_family_loss: 0.5768839624403017, total_identity_loss: 0.9050575096378005\n",
            "Epoch 11, total_loss: 0.10392803154272887, total_tri_loss: 0.040030669066519635, total_family_loss: 0.5126858620270275, total_identity_loss: 0.7652613488132111\n",
            "Epoch 12, total_loss: 0.09227245853783708, total_tri_loss: 0.03648666206477773, total_family_loss: 0.45344726220585546, total_identity_loss: 0.6622686417653374\n",
            "Epoch 13, total_loss: 0.08675195493239714, total_tri_loss: 0.03670238001702208, total_family_loss: 0.41197222087262764, total_identity_loss: 0.5890192580648409\n",
            "Epoch 14, total_loss: 0.07875318238207087, total_tri_loss: 0.034337167338467214, total_family_loss: 0.37732587901522785, total_identity_loss: 0.5109943977668807\n",
            "Epoch 15, total_loss: 0.07221218441180124, total_tri_loss: 0.03268585104801728, total_family_loss: 0.34498258088341555, total_identity_loss: 0.44554407661568424\n",
            "Epoch 16, total_loss: 0.06940416333637578, total_tri_loss: 0.033958867797766495, total_family_loss: 0.31127723890797, total_identity_loss: 0.397628655441925\n",
            "Epoch 17, total_loss: 0.06552549747545254, total_tri_loss: 0.033408385154867784, total_family_loss: 0.28677539037996524, total_identity_loss: 0.35556684011155953\n",
            "Epoch 18, total_loss: 0.058087672043602816, total_tri_loss: 0.02892866528734225, total_family_loss: 0.26373754426083784, total_identity_loss: 0.3194425724779059\n",
            "Epoch 19, total_loss: 0.05424308948610181, total_tri_loss: 0.027401048146609153, total_family_loss: 0.24831045293595322, total_identity_loss: 0.2885303698103539\n",
            "Epoch 20, total_loss: 0.054886944425117865, total_tri_loss: 0.029216851716412542, total_family_loss: 0.23976063168911324, total_identity_loss: 0.27364120827203225\n",
            "Epoch 21, total_loss: 0.05300586479979062, total_tri_loss: 0.029974793123655443, total_family_loss: 0.2169355880487544, total_identity_loss: 0.24368583396948482\n",
            "Epoch 22, total_loss: 0.05249074165246886, total_tri_loss: 0.03116126511729626, total_family_loss: 0.202303468572609, total_identity_loss: 0.224286056427346\n",
            "Epoch 23, total_loss: 0.046423479610439335, total_tri_loss: 0.026219515153553016, total_family_loss: 0.19481412822415736, total_identity_loss: 0.2092651544613219\n",
            "Epoch 24, total_loss: 0.0453666557981545, total_tri_loss: 0.026335185810524835, total_family_loss: 0.18262106253441546, total_identity_loss: 0.19800832890956205\n",
            "Epoch 25, total_loss: 0.042882485935839244, total_tri_loss: 0.02497344368171571, total_family_loss: 0.17375376651172478, total_identity_loss: 0.1844270703134499\n",
            "Epoch 26, total_loss: 0.040989769635605744, total_tri_loss: 0.023833699524771823, total_family_loss: 0.1691975838276747, total_identity_loss: 0.17392380975696328\n",
            "Epoch 27, total_loss: 0.040597276797202465, total_tri_loss: 0.0246210243120952, total_family_loss: 0.15541624538194318, total_identity_loss: 0.16410879889730184\n",
            "Epoch 28, total_loss: 0.03668895982449544, total_tri_loss: 0.021195016265269904, total_family_loss: 0.15251302618396767, total_identity_loss: 0.15736583963671572\n",
            "Epoch 29, total_loss: 0.03645466493499881, total_tri_loss: 0.021505768612448063, total_family_loss: 0.14635485828040967, total_identity_loss: 0.15262305793251815\n",
            "Epoch 30, total_loss: 0.036633892268739914, total_tri_loss: 0.023259390999144914, total_family_loss: 0.1313824782345529, total_identity_loss: 0.1361075416414069\n",
            "Epoch 31, total_loss: 0.033624887188787624, total_tri_loss: 0.020513815852205155, total_family_loss: 0.1284691732867858, total_identity_loss: 0.13375224900700527\n",
            "Epoch 32, total_loss: 0.03408220862550707, total_tri_loss: 0.021025622496519854, total_family_loss: 0.12813907426341675, total_identity_loss: 0.1329926483813079\n",
            "Epoch 33, total_loss: 0.0336268123132209, total_tri_loss: 0.020845056955432158, total_family_loss: 0.12290972483937875, total_identity_loss: 0.13272537266239304\n",
            "Epoch 34, total_loss: 0.033833478628939276, total_tri_loss: 0.021006445886845984, total_family_loss: 0.12542414332641247, total_identity_loss: 0.13111650633032187\n",
            "Epoch 35, total_loss: 0.030729496050755263, total_tri_loss: 0.018129663761036897, total_family_loss: 0.12253864688874473, total_identity_loss: 0.12945799501049743\n",
            "Epoch 36, total_loss: 0.034064353494448044, total_tri_loss: 0.021307022479705225, total_family_loss: 0.1253942267864027, total_identity_loss: 0.12975238889780696\n",
            "Epoch 37, total_loss: 0.030225460012734115, total_tri_loss: 0.01770389798005934, total_family_loss: 0.12257657497678213, total_identity_loss: 0.1278546618960421\n",
            "Epoch 38, total_loss: 0.03213155965982568, total_tri_loss: 0.019635349813187854, total_family_loss: 0.1218225799847996, total_identity_loss: 0.12810161210323348\n",
            "Epoch 39, total_loss: 0.030483175408763385, total_tri_loss: 0.018012490649259954, total_family_loss: 0.12110843187044823, total_identity_loss: 0.12830525676842605\n",
            "Epoch 40, total_loss: 0.03237537967762933, total_tri_loss: 0.01986515679580369, total_family_loss: 0.12180280276639728, total_identity_loss: 0.12840164672542728\n",
            "Epoch 41, total_loss: 0.033034530987942776, total_tri_loss: 0.020433947496020167, total_family_loss: 0.12148460087738377, total_identity_loss: 0.13052706179699175\n",
            "Epoch 42, total_loss: 0.030593816448663932, total_tri_loss: 0.018215711803957303, total_family_loss: 0.1217020842524539, total_identity_loss: 0.12586000471037137\n",
            "Epoch 43, total_loss: 0.03192799428830475, total_tri_loss: 0.019542601108883312, total_family_loss: 0.11980627505081023, total_identity_loss: 0.12790158452877795\n",
            "Epoch 44, total_loss: 0.03102016275964596, total_tri_loss: 0.018761563255535615, total_family_loss: 0.11926215575459691, total_identity_loss: 0.12590983101380002\n",
            "Epoch 45, total_loss: 0.029227462868742476, total_tri_loss: 0.017030397863510342, total_family_loss: 0.12003678349395928, total_identity_loss: 0.1239045111346174\n",
            "Epoch 46, total_loss: 0.028174398718558702, total_tri_loss: 0.01599834393954497, total_family_loss: 0.11797461686332823, total_identity_loss: 0.12554647346554237\n",
            "Epoch 47, total_loss: 0.03194245521064083, total_tri_loss: 0.0196377451768804, total_family_loss: 0.12167237416072219, total_identity_loss: 0.12442182302031927\n",
            "Epoch 48, total_loss: 0.030027736311388914, total_tri_loss: 0.01789512823310905, total_family_loss: 0.11768349774322141, total_identity_loss: 0.12496865869188686\n",
            "Epoch 49, total_loss: 0.02875276641925747, total_tri_loss: 0.01657106185274714, total_family_loss: 0.11934504586357782, total_identity_loss: 0.12428904007991076\n",
            "Epoch 50, total_loss: 0.031968034304331386, total_tri_loss: 0.019882666295333383, total_family_loss: 0.11674903419581348, total_identity_loss: 0.12495832251840824\n",
            "Epoch 51, total_loss: 0.029669393439536288, total_tri_loss: 0.01796206446896179, total_family_loss: 0.11416177691151531, total_identity_loss: 0.11998479778631946\n",
            "Epoch 52, total_loss: 0.031136634983528477, total_tri_loss: 0.01932124566799999, total_family_loss: 0.1146501072600292, total_identity_loss: 0.1216576762135603\n",
            "Epoch 53, total_loss: 0.029432086078717758, total_tri_loss: 0.017424778886161545, total_family_loss: 0.11587384323062462, total_identity_loss: 0.12427229327772725\n",
            "Epoch 54, total_loss: 0.0302471318344029, total_tri_loss: 0.018180583592050138, total_family_loss: 0.11953226370724035, total_identity_loss: 0.12179869613485601\n",
            "Epoch 55, total_loss: 0.031031489344562953, total_tri_loss: 0.019168653433110538, total_family_loss: 0.11570733522163747, total_identity_loss: 0.12154937664451164\n",
            "Epoch 56, total_loss: 0.029055485922411247, total_tri_loss: 0.017034480639163872, total_family_loss: 0.1186448060456069, total_identity_loss: 0.12177529563929801\n",
            "Epoch 57, total_loss: 0.029724238127676772, total_tri_loss: 0.01792288480014554, total_family_loss: 0.11587751680104537, total_identity_loss: 0.12014954245480367\n",
            "Epoch 58, total_loss: 0.030389654979744798, total_tri_loss: 0.018742355174865902, total_family_loss: 0.1137993598598673, total_identity_loss: 0.11914663097283058\n",
            "Epoch 59, total_loss: 0.030663758370106, total_tri_loss: 0.018884945832510415, total_family_loss: 0.11600769708012447, total_identity_loss: 0.11956854672332703\n",
            "Epoch 60, total_loss: 0.030244793675699603, total_tri_loss: 0.018511113434258344, total_family_loss: 0.11434892597059074, total_identity_loss: 0.12032467317652064\n",
            "Epoch 61, total_loss: 0.031502191595386114, total_tri_loss: 0.0198847779123587, total_family_loss: 0.11275947985670375, total_identity_loss: 0.11958878979278863\n",
            "Epoch 62, total_loss: 0.03040311550066185, total_tri_loss: 0.018714578893511927, total_family_loss: 0.11528670272842508, total_identity_loss: 0.11848402432100506\n",
            "Epoch 63, total_loss: 0.02811755889397192, total_tri_loss: 0.016343883131847627, total_family_loss: 0.11509299810211533, total_identity_loss: 0.1203805119714841\n",
            "Epoch 64, total_loss: 0.03052323073133959, total_tri_loss: 0.019003598111413042, total_family_loss: 0.11285187508707831, total_identity_loss: 0.1175407730257074\n",
            "Epoch 65, total_loss: 0.02781673591691037, total_tri_loss: 0.016110130460015373, total_family_loss: 0.11387025378681387, total_identity_loss: 0.12026185118939406\n",
            "Epoch 66, total_loss: 0.030091614068676862, total_tri_loss: 0.018578774516717175, total_family_loss: 0.11053895057724536, total_identity_loss: 0.1197178345989543\n",
            "Epoch 67, total_loss: 0.028411747599380578, total_tri_loss: 0.016847598820932144, total_family_loss: 0.11208794758506525, total_identity_loss: 0.11919502262229136\n",
            "Epoch 68, total_loss: 0.027118306997258906, total_tri_loss: 0.015514787538687822, total_family_loss: 0.11258404702834261, total_identity_loss: 0.11948633978754371\n",
            "Epoch 69, total_loss: 0.030392485828581013, total_tri_loss: 0.018994510804586355, total_family_loss: 0.11055672669670627, total_identity_loss: 0.11740276758569201\n",
            "Epoch 70, total_loss: 0.030662157821835446, total_tri_loss: 0.018966788299126982, total_family_loss: 0.11365401879824306, total_identity_loss: 0.1202533688015933\n",
            "Epoch 71, total_loss: 0.029774898117845665, total_tri_loss: 0.018212338748902112, total_family_loss: 0.1123963857222832, total_identity_loss: 0.11885479512606906\n",
            "Epoch 72, total_loss: 0.029677009144641008, total_tri_loss: 0.018203878523994188, total_family_loss: 0.11080960277818712, total_identity_loss: 0.11865300749409187\n",
            "Epoch 73, total_loss: 0.029205804405322278, total_tri_loss: 0.0176304202179754, total_family_loss: 0.11384663243359924, total_identity_loss: 0.11766104625226012\n",
            "Epoch 74, total_loss: 0.028606911720588022, total_tri_loss: 0.017076510696037793, total_family_loss: 0.11232702588243457, total_identity_loss: 0.11828099101397871\n",
            "Epoch 75, total_loss: 0.028323185176572903, total_tri_loss: 0.016806373117183315, total_family_loss: 0.10981879714281992, total_identity_loss: 0.12051744297427394\n",
            "Epoch 76, total_loss: 0.027995255475769667, total_tri_loss: 0.016710132096106754, total_family_loss: 0.10963827512191002, total_identity_loss: 0.11606418478742936\n",
            "Epoch 77, total_loss: 0.028966323511141496, total_tri_loss: 0.017541946315855466, total_family_loss: 0.11120305235014917, total_identity_loss: 0.11728448909739674\n",
            "Epoch 78, total_loss: 0.029459074250356883, total_tri_loss: 0.017934033979834365, total_family_loss: 0.11119974738335822, total_identity_loss: 0.11930105595215343\n",
            "Epoch 79, total_loss: 0.029870020003381993, total_tri_loss: 0.018382310329929985, total_family_loss: 0.11119054542453839, total_identity_loss: 0.11856364520087587\n",
            "Epoch 80, total_loss: 0.028112088157116952, total_tri_loss: 0.016295009373333693, total_family_loss: 0.11553703749587443, total_identity_loss: 0.12080453095368753\n",
            "Epoch 81, total_loss: 0.02847684475051459, total_tri_loss: 0.01704219861502498, total_family_loss: 0.11241336909110766, total_identity_loss: 0.11627954754708898\n",
            "Epoch 82, total_loss: 0.029803389629830927, total_tri_loss: 0.018275287149822926, total_family_loss: 0.11163221945611407, total_identity_loss: 0.11892982511028899\n",
            "Epoch 83, total_loss: 0.028180995450620017, total_tri_loss: 0.016875300464488943, total_family_loss: 0.10957693459227962, total_identity_loss: 0.1165369607294043\n",
            "Epoch 84, total_loss: 0.030616521946827412, total_tri_loss: 0.01905597470353687, total_family_loss: 0.11206900515511439, total_identity_loss: 0.11914193875371405\n",
            "Epoch 85, total_loss: 0.02875337292162408, total_tri_loss: 0.017422069926959407, total_family_loss: 0.11024749714390138, total_identity_loss: 0.1163785571946852\n",
            "Epoch 86, total_loss: 0.027484686295728027, total_tri_loss: 0.016164688266282097, total_family_loss: 0.10991935340541796, total_identity_loss: 0.11648060075414429\n",
            "Epoch 87, total_loss: 0.028143809311187894, total_tri_loss: 0.016673236601405413, total_family_loss: 0.11145939045429702, total_identity_loss: 0.11795205964057959\n",
            "Epoch 88, total_loss: 0.02736083175008609, total_tri_loss: 0.015853162494933876, total_family_loss: 0.11221212302551516, total_identity_loss: 0.11794125837365274\n",
            "Epoch 89, total_loss: 0.028794216740104794, total_tri_loss: 0.01717640114920463, total_family_loss: 0.1118670643808466, total_identity_loss: 0.12048924593374916\n",
            "Epoch 90, total_loss: 0.02784429591581063, total_tri_loss: 0.016037672172541425, total_family_loss: 0.11376581684980686, total_identity_loss: 0.12236665519188843\n",
            "Epoch 91, total_loss: 0.02782322908719361, total_tri_loss: 0.01611253660452821, total_family_loss: 0.11335574593191931, total_identity_loss: 0.12085809921338135\n",
            "Epoch 92, total_loss: 0.028506508711042444, total_tri_loss: 0.017093247539992376, total_family_loss: 0.11215811177453154, total_identity_loss: 0.1161071084949259\n",
            "Epoch 93, total_loss: 0.029957591717171127, total_tri_loss: 0.01811195096837281, total_family_loss: 0.11639997150427758, total_identity_loss: 0.12051283918742027\n",
            "Epoch 94, total_loss: 0.030051407060697602, total_tri_loss: 0.018404470828275968, total_family_loss: 0.11267885699853435, total_identity_loss: 0.12025986213691171\n",
            "Epoch 95, total_loss: 0.02941564583248201, total_tri_loss: 0.017942017174489257, total_family_loss: 0.11034711816443206, total_identity_loss: 0.11912545102941033\n",
            "Epoch 96, total_loss: 0.029398843689615827, total_tri_loss: 0.017916766467722958, total_family_loss: 0.11277202006551272, total_identity_loss: 0.11686952100370755\n",
            "Epoch 97, total_loss: 0.027443079964596523, total_tri_loss: 0.016079021175091992, total_family_loss: 0.11052575272806099, total_identity_loss: 0.11675541965056459\n",
            "Epoch 98, total_loss: 0.030861677379302156, total_tri_loss: 0.019335135242673756, total_family_loss: 0.11233546749510542, total_identity_loss: 0.1181953724063896\n",
            "Epoch 99, total_loss: 0.028209321045781034, total_tri_loss: 0.016592017689681028, total_family_loss: 0.11299389020361678, total_identity_loss: 0.11935217047647395\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 279
        },
        "id": "ZEBPG2bqqEuk",
        "outputId": "1eee5fcc-3c67-4268-d1b5-8bc3940f6231"
      },
      "source": [
        "torch.save(model.state_dict(), 'model.pth.tar') # download this tar file from colab to my local drive (to use it for new_test.ipynb)\n",
        "\n",
        "x = list(range(num_epoch))\n",
        "plt.plot(x, train_losses, marker='.', c='red', label=\"Train-set Loss\")\n",
        "\n",
        "plt.legend(loc='upper right')\n",
        "plt.grid()\n",
        "plt.xlabel('epoch')\n",
        "plt.ylabel('loss')\n",
        "plt.show()"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    }
  ]
}