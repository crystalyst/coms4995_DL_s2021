{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "new_test.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j4cTAMZPmN53",
        "outputId": "32aab278-1fec-423e-fc53-7e925e255ab7"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/gdrive')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M9JZZaRruv9K"
      },
      "source": [
        "MODEL_PATH = 'model.pth.tar'"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9l88RDSmDfAV"
      },
      "source": [
        "import os\n",
        "import math\n",
        "import pickle\n",
        "import cv2\n",
        "from glob import glob\n",
        "import time\n",
        "\n",
        "import torch\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "\n",
        "from PIL import Image\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torchvision import transforms, models\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.image as img"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hP3_ahu8xFaq"
      },
      "source": [
        "# Reference: https://github.com/cydonia999/VGGFace2-pytorch\n",
        "# ZQ. Cao, L. Shen, W. Xie, O. M. Parkhi, A. Zisserman, VGGFace2: A dataset for recognising faces across pose and age, 2018.\n",
        "# https://arxiv.org/pdf/1710.08092.pdf\n",
        "\n",
        "def conv3x3(in_planes, out_planes, stride=1):\n",
        "    \"\"\"3x3 convolution with padding\"\"\"\n",
        "    return nn.Conv2d(in_planes, out_planes, kernel_size=3, stride=stride,\n",
        "                     padding=1, bias=False)\n",
        "\n",
        "# This SEModule is not used.\n",
        "class SEModule(nn.Module):\n",
        "\n",
        "    def __init__(self, planes, compress_rate):\n",
        "        super(SEModule, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(planes, planes // compress_rate, kernel_size=1, stride=1, bias=True)\n",
        "        self.conv2 = nn.Conv2d(planes // compress_rate, planes, kernel_size=1, stride=1, bias=True)\n",
        "        self.relu = nn.ReLU(inplace=True)\n",
        "        self.sigmoid = nn.Sigmoid()\n",
        "\n",
        "    def forward(self, x):\n",
        "        module_input = x\n",
        "        x = F.avg_pool2d(module_input, kernel_size=module_input.size(2))\n",
        "        x = self.conv1(x)\n",
        "        x = self.relu(x)\n",
        "        x = self.conv2(x)\n",
        "        x = self.sigmoid(x)\n",
        "        return module_input * x\n",
        "\n",
        "\n",
        "class BasicBlock(nn.Module):\n",
        "    expansion = 1\n",
        "\n",
        "    def __init__(self, inplanes, planes, stride=1, downsample=None):\n",
        "        super(BasicBlock, self).__init__()\n",
        "        self.conv1 = conv3x3(inplanes, planes, stride)\n",
        "        self.bn1 = nn.BatchNorm2d(planes)\n",
        "        self.relu = nn.ReLU(inplace=True)\n",
        "        self.conv2 = conv3x3(planes, planes)\n",
        "        self.bn2 = nn.BatchNorm2d(planes)\n",
        "        self.downsample = downsample\n",
        "        self.stride = stride\n",
        "\n",
        "    def forward(self, x):\n",
        "        residual = x\n",
        "\n",
        "        out = self.conv1(x)\n",
        "        out = self.bn1(out)\n",
        "        out = self.relu(out)\n",
        "\n",
        "        out = self.conv2(out)\n",
        "        out = self.bn2(out)\n",
        "\n",
        "        if self.downsample is not None:\n",
        "            residual = self.downsample(x)\n",
        "\n",
        "        out += residual\n",
        "        out = self.relu(out)\n",
        "\n",
        "        return out\n",
        "\n",
        "\n",
        "class Bottleneck(nn.Module):\n",
        "    expansion = 4\n",
        "\n",
        "    def __init__(self, inplanes, planes, stride=1, downsample=None):\n",
        "        super(Bottleneck, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(inplanes, planes, kernel_size=1, stride=stride, bias=False)\n",
        "        self.bn1 = nn.BatchNorm2d(planes)\n",
        "        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3, stride=1, padding=1, bias=False)\n",
        "        self.bn2 = nn.BatchNorm2d(planes)\n",
        "        self.conv3 = nn.Conv2d(planes, planes * 4, kernel_size=1, bias=False)\n",
        "        self.bn3 = nn.BatchNorm2d(planes * 4)\n",
        "        self.relu = nn.ReLU(inplace=True)\n",
        "        self.downsample = downsample\n",
        "        self.stride = stride\n",
        "\n",
        "        # SENet\n",
        "        compress_rate = 16\n",
        "        # self.se_block = SEModule(planes * 4, compress_rate)  # this is not used.\n",
        "        self.conv4 = nn.Conv2d(planes * 4, planes * 4 // compress_rate, kernel_size=1, stride=1, bias=True)\n",
        "        self.conv5 = nn.Conv2d(planes * 4 // compress_rate, planes * 4, kernel_size=1, stride=1, bias=True)\n",
        "        self.sigmoid = nn.Sigmoid()\n",
        "\n",
        "    def forward(self, x):\n",
        "        residual = x\n",
        "\n",
        "        out = self.conv1(x)\n",
        "        out = self.bn1(out)\n",
        "        out = self.relu(out)\n",
        "\n",
        "        out = self.conv2(out)\n",
        "        out = self.bn2(out)\n",
        "        out = self.relu(out)\n",
        "\n",
        "        out = self.conv3(out)\n",
        "        out = self.bn3(out)\n",
        "\n",
        "\n",
        "        ## senet\n",
        "        out2 = F.avg_pool2d(out, kernel_size=out.size(2))\n",
        "        out2 = self.conv4(out2)\n",
        "        out2 = self.relu(out2)\n",
        "        out2 = self.conv5(out2)\n",
        "        out2 = self.sigmoid(out2)\n",
        "        # out2 = self.se_block.forward(out)  # not used\n",
        "\n",
        "        if self.downsample is not None:\n",
        "            residual = self.downsample(x)\n",
        "\n",
        "        out = out2 * out + residual\n",
        "        # out = out2 + residual  # not used\n",
        "        out = self.relu(out)\n",
        "        return out\n",
        "\n",
        "\n",
        "class SENet(nn.Module):\n",
        "\n",
        "    def __init__(self, block, layers, num_classes=1000, include_top=True):\n",
        "        self.inplanes = 64\n",
        "        super(SENet, self).__init__()\n",
        "        self.include_top = include_top\n",
        "        \n",
        "        self.conv1 = nn.Conv2d(3, 64, kernel_size=7, stride=2, padding=3, bias=False)\n",
        "        self.bn1 = nn.BatchNorm2d(64)\n",
        "        self.relu = nn.ReLU(inplace=True)\n",
        "        self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=0, ceil_mode=True)\n",
        "\n",
        "        self.layer1 = self._make_layer(block, 64, layers[0])\n",
        "        self.layer2 = self._make_layer(block, 128, layers[1], stride=2)\n",
        "        self.layer3 = self._make_layer(block, 256, layers[2], stride=2)\n",
        "        self.layer4 = self._make_layer(block, 512, layers[3], stride=2)\n",
        "        self.avgpool = nn.AvgPool2d(7, stride=1)\n",
        "        self.fc = nn.Linear(512 * block.expansion, num_classes)\n",
        "\n",
        "        for m in self.modules():\n",
        "            if isinstance(m, nn.Conv2d):\n",
        "                n = m.kernel_size[0] * m.kernel_size[1] * m.out_channels\n",
        "                m.weight.data.normal_(0, math.sqrt(2. / n))\n",
        "            elif isinstance(m, nn.BatchNorm2d):\n",
        "                m.weight.data.fill_(1)\n",
        "                m.bias.data.zero_()\n",
        "\n",
        "    def _make_layer(self, block, planes, blocks, stride=1):\n",
        "        downsample = None\n",
        "        if stride != 1 or self.inplanes != planes * block.expansion:\n",
        "            downsample = nn.Sequential(\n",
        "                nn.Conv2d(self.inplanes, planes * block.expansion,\n",
        "                          kernel_size=1, stride=stride, bias=False),\n",
        "                nn.BatchNorm2d(planes * block.expansion),\n",
        "            )\n",
        "\n",
        "        layers = []\n",
        "        layers.append(block(self.inplanes, planes, stride, downsample))\n",
        "        self.inplanes = planes * block.expansion\n",
        "        for i in range(1, blocks):\n",
        "            layers.append(block(self.inplanes, planes))\n",
        "\n",
        "        return nn.Sequential(*layers)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.conv1(x)\n",
        "        x = self.bn1(x)\n",
        "        x = self.relu(x)\n",
        "        x = self.maxpool(x)\n",
        "        x = self.layer1(x)\n",
        "        x = self.layer2(x)\n",
        "        x = self.layer3(x)\n",
        "        x = self.layer4(x)\n",
        "\n",
        "        x = self.avgpool(x)\n",
        "        \n",
        "        if not self.include_top:\n",
        "            return x\n",
        "        \n",
        "        x = x.view(x.size(0), -1)\n",
        "        x = self.fc(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "def senet50(**kwargs):\n",
        "    \"\"\"Constructs a SENet-50 model.\n",
        "    \"\"\"\n",
        "    model = SENet(Bottleneck, [3, 4, 6, 3], **kwargs)\n",
        "    return model\n",
        "\n",
        "\n",
        "def load_state_dict(model, fname):\n",
        "    \"\"\"\n",
        "    Set parameters converted from Caffe models authors of VGGFace2 provide.\n",
        "    See https://www.robots.ox.ac.uk/~vgg/data/vgg_face2/.\n",
        "    Arguments:\n",
        "        model: model\n",
        "        fname: file name of parameters converted from a Caffe model, assuming the file format is Pickle.\n",
        "    \"\"\"\n",
        "    with open(fname, 'rb') as f:\n",
        "        weights = pickle.load(f, encoding='latin1')\n",
        "\n",
        "    own_state = model.state_dict()\n",
        "    for name, param in weights.items():\n",
        "        if name in own_state:\n",
        "            try:\n",
        "                own_state[name].copy_(torch.from_numpy(param))\n",
        "            except Exception:\n",
        "                raise RuntimeError('While copying the parameter named {}, whose dimensions in the model are {} and whose '\\\n",
        "                                   'dimensions in the checkpoint are {}.'.format(name, own_state[name].size(), param.size()))\n",
        "        else:\n",
        "            raise KeyError('unexpected key \"{}\" in state_dict'.format(name))"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wyvHHYp6Dl1N"
      },
      "source": [
        "class BaseModel(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(BaseModel, self).__init__()\n",
        "        N_IDENTITY = 8631\n",
        "        include_top = True\n",
        "        self.backbone = senet50(num_classes=N_IDENTITY, include_top=include_top)\n",
        "        load_state_dict(self.backbone, '/gdrive/MyDrive/Kinship Recognition Starter/senet50_ft_weight.pkl')\n",
        "        self.backbone.fc = nn.Linear(2048, 1024) # reset top layer\n",
        "        self.embedding = nn.Linear(1024, 512)\n",
        "        self.family_classifier = nn.Linear(1024, 192) \n",
        "        self.identity_classifier = nn.Linear(1024, 966)\n",
        "        \n",
        "    def forward(self, x):\n",
        "        feature = self.backbone(x)\n",
        "        feature = F.relu(feature)\n",
        "        \n",
        "        embedding = self.embedding(feature)\n",
        "        embedding = F.normalize(embedding, p=2)\n",
        "        \n",
        "        family_pred = self.family_classifier(feature)\n",
        "        identity_pred = self.identity_classifier(feature)\n",
        "        \n",
        "        return embedding, family_pred, identity_pred"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5KEeBV_1mf1-"
      },
      "source": [
        "model = BaseModel()\n",
        "\n",
        "if torch.cuda.is_available():\n",
        "    model.cuda()\n",
        "\n",
        "model.load_state_dict(torch.load(MODEL_PATH))\n",
        "model.eval()\n",
        "\n",
        "# Since it might take too much time to probe images in the train set, I intended to use images that belong to train pairs\n",
        "train_pair = pd.read_csv('/gdrive/MyDrive/Kinship Recognition Starter/train_ds.csv')\n",
        "train_pair_id = []\n",
        "train_pair_id += list(train_pair.values[:, 1])\n",
        "train_pair_id += list(train_pair.values[:, 2])\n",
        "train_pair_id = list(set(train_pair_id))"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2uCgpQpwmQC-",
        "outputId": "21f76c8e-5ca5-49de-e686-47dfec87b81b"
      },
      "source": [
        "class ImageDataset(Dataset):\n",
        "    def __init__(self, data_root, transform=None, is_train=False):\n",
        "        # Safe the entire path for data (list and label as well)\n",
        "        super(ImageDataset, self).__init__()\n",
        "        self.transform = transform\n",
        "\n",
        "        self.data_root = data_root\n",
        "        self.is_train = is_train\n",
        "        if is_train:\n",
        "            self.all_images = train_pair_id\n",
        "        else:\n",
        "            self.all_images = glob(data_root + \"*.jpg\")\n",
        "\n",
        "        self.img_pathes = []\n",
        "\n",
        "    def __len__(self):\n",
        "        # size of the entire dataset\n",
        "        return len(self.all_images)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        # function that returns data x, label (one by one) when there is an idx input\n",
        "        if self.is_train:\n",
        "            img_path = self.data_root + self.all_images[idx]\n",
        "        else:   \n",
        "            img_path = self.all_images[idx]\n",
        "        img = Image.open(img_path).convert('RGB')\n",
        "\n",
        "        if self.transform is not None:\n",
        "            img = self.transform(img)\n",
        "\n",
        "        if img.size(0) == 1:\n",
        "            img = img.repeat(3, 1, 1)\n",
        "\n",
        "        return img, img_path[len(self.data_root):]\n",
        "\n",
        "# bring a vector in the embedding space for each image\n",
        "def get_embeddings(model, image_loader):\n",
        "    with torch.no_grad():\n",
        "        id_list = []\n",
        "        embed_list = []\n",
        "        for batch_idx, (data, image_path) in enumerate(image_loader):\n",
        "            data = data.cuda()\n",
        "            embedding, _, _ = model(data)  # size : Batch x 512\n",
        "\n",
        "            if batch_idx == 0:\n",
        "                embed_list = embedding.cpu().numpy()\n",
        "                id_list = image_path\n",
        "            else:\n",
        "                embed_list = np.concatenate((embed_list, embedding.cpu().numpy()), axis=0)\n",
        "                id_list = np.concatenate((id_list, image_path), axis=0)\n",
        "\n",
        "            del embedding\n",
        "\n",
        "    id_list = list(id_list)\n",
        "    return embed_list, id_list\n",
        "\n",
        "\n",
        "train_pair_dataset = ImageDataset(data_root= '/gdrive/MyDrive/Kinship Recognition Starter/train/train-faces/', transform=transforms.Compose([\n",
        "        transforms.Resize(256),\n",
        "        transforms.CenterCrop(224),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize((131.0912/255., 103.8827/255., 91.4953/255.), (1, 1, 1))\n",
        "    ]), is_train=True)\n",
        "\n",
        "\n",
        "train_pair_loader = DataLoader(train_pair_dataset, num_workers=2, batch_size=32, shuffle=False)\n",
        "\n",
        "train_pair_embed_list, train_pair_id_list = get_embeddings(model, train_pair_loader)\n",
        "\n",
        "distances_list = []\n",
        "labels = []\n",
        "\n",
        "for i in range(len(train_pair)):\n",
        "    id1, id2 = train_pair.values[i][1], train_pair.values[i][2]\n",
        "    idx1, idx2 = train_pair_id_list.index(id1), train_pair_id_list.index(id2)\n",
        "    embed1, embed2 = train_pair_embed_list[idx1], train_pair_embed_list[idx2]\n",
        "    label = train_pair.values[i][3]\n",
        "    distance = np.linalg.norm(embed1 - embed2)\n",
        "\n",
        "    distances_list.append(distance)\n",
        "    labels.append(label)\n",
        "    \n",
        "    if i % 1000 == 0:\n",
        "        print (i, len(train_pair))\n",
        "# Since we know this dataset is well balanced, I sorted the distance to create predictions (lowest half - positive / highest half - negative )\n",
        "predictions = np.zeros_like(distances_list)\n",
        "pos_list = np.argsort(distances_list)[:len(predictions)//2]\n",
        "predictions[pos_list] = 1\n",
        "correct = sum(predictions==labels)\n",
        "\n",
        "# train pair accuracy is meaningful\n",
        "print('train pair accuracy:', correct / len(train_pair))\n"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:718: UserWarning: Named tensors and all their associated APIs are an experimental feature and subject to change. Please do not use them for anything important until they are released as stable. (Triggered internally at  /pytorch/c10/core/TensorImpl.h:1156.)\n",
            "  return torch.max_pool2d(input, kernel_size, stride, padding, dilation, ceil_mode)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "0 30000\n",
            "1000 30000\n",
            "2000 30000\n",
            "3000 30000\n",
            "4000 30000\n",
            "5000 30000\n",
            "6000 30000\n",
            "7000 30000\n",
            "8000 30000\n",
            "9000 30000\n",
            "10000 30000\n",
            "11000 30000\n",
            "12000 30000\n",
            "13000 30000\n",
            "14000 30000\n",
            "15000 30000\n",
            "16000 30000\n",
            "17000 30000\n",
            "18000 30000\n",
            "19000 30000\n",
            "20000 30000\n",
            "21000 30000\n",
            "22000 30000\n",
            "23000 30000\n",
            "24000 30000\n",
            "25000 30000\n",
            "26000 30000\n",
            "27000 30000\n",
            "28000 30000\n",
            "29000 30000\n",
            "train pair accuracy: 0.9340666666666667\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DVNbNhZbm4cL"
      },
      "source": [
        "test_dataset = ImageDataset(data_root= '/gdrive/MyDrive/Kinship Recognition Starter/test/', transform=transforms.Compose([\n",
        "        transforms.Resize(256),\n",
        "        transforms.CenterCrop(224),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize((131.0912/255., 103.8827/255., 91.4953/255.), (1, 1, 1))\n",
        "    ]))\n",
        "\n",
        "test_loader = DataLoader(test_dataset, num_workers=2, batch_size=128, shuffle=False)\n",
        "\n",
        "embed_list, id_list = get_embeddings(model, test_loader)\n",
        "\n",
        "submission = pd.read_csv('/gdrive/MyDrive/Kinship Recognition Starter/test_ds.csv')\n",
        "\n",
        "distances_list = []\n",
        "\n",
        "for i in range(len(submission)):\n",
        "    id1, id2 = submission.values[i][1], submission.values[i][2]\n",
        "    idx1, idx2 = id_list.index(id1), id_list.index(id2)\n",
        "    embed1, embed2 = embed_list[idx1], embed_list[idx2]\n",
        "    distance = np.linalg.norm(embed1 - embed2)\n",
        "    #print(distance)\n",
        "\n",
        "    if distance < 0.5:\n",
        "        result = 1\n",
        "    else:\n",
        "        result = 0\n",
        "\n",
        "    #predictions1.append(result)\n",
        "    distances_list.append(distance)\n",
        "\n",
        "#d = {'index': np.arange(0, 3000, 1), 'label':predictions1}\n",
        "#submissionfile = pd.DataFrame(data=d)\n",
        "#submissionfile = submissionfile.round()\n",
        "#submissionfile.to_csv(\"predictions1.csv\", index=False)\n",
        "\n",
        "#submissionfile.astype(\"int64\").to_csv(\"predictions1.csv\", index=False)\n",
        "\n",
        "predictions = np.zeros_like(distances_list)\n",
        "pos_list = np.argsort(distances_list)[:len(predictions)//2]\n",
        "predictions[pos_list] = 1\n",
        "\n",
        "d = {'index': np.arange(0, 3000, 1), 'label':predictions}\n",
        "submissionfile = pd.DataFrame(data=d)\n",
        "#submissionfile = submissionfile.round()\n",
        "#submissionfile.to_csv(\"predictions.csv\", index=False)\n",
        "\n",
        "#predictions -> This is the file to submit\n",
        "submissionfile.astype(\"int64\").to_csv(\"predictions.csv\", index=False)"
      ],
      "execution_count": 12,
      "outputs": []
    }
  ]
}